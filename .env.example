# Environment variables for Quizly Application

# LLM Provider Configuration
# Choose your LLM provider: "ollama" or "openai"
# Defaults to "ollama" if not specified or invalid.
LLM_PROVIDER="ollama"

# Ollama Configuration (only used if LLM_PROVIDER="ollama")
OLLAMA_API_HOST="http://localhost:11434"
OLLAMA_MODEL="mistral" # Or any other model you have pulled, e.g., "llama3.2"

# OpenAI Configuration (only used if LLM_PROVIDER="openai")
# Ensure you have set your OpenAI API key.
OPENAI_API_KEY="your_openai_api_key_here"
OPENAI_MODEL="gpt-3.5-turbo" # Or other compatible models like "gpt-4"

# Prompt Template for AI Question Generation
# This template is used by both Ollama and OpenAI providers.
# Ensure it instructs the LLM to return JSON in the expected format.
# Variables available: {subject}, {limit}, {subject_lowercase}
# Note: OpenAIProvider's prompt is slightly different in its example to request a root "questions" key.
# The default in llm_providers.py for OpenAIProvider is tuned for its JSON mode.
# You can override it here if needed, but ensure it's compatible with the chosen provider's parsing logic.
PROMPT_TEMPLATE="Generate {limit} multiple-choice quiz questions about {subject}. \
Each question should have exactly 4 answer options labeled a, b, c, d.\
Format your response as a JSON array where each question has this structure:\
{{\n\
    \"text\": \"question text here?\",\n\
    \"options\": [\n\
        {{\"id\": \"a\", \"text\": \"option A text\"}},\n\
        {{\"id\": \"b\", \"text\": \"option B text\"}},\n\
        {{\"id\": \"c\", \"text\": \"option C text\"}},\n\
        {{\"id\": \"d\", \"text\": \"option D text\"}}\n\
    ],\n\
    \"correct_answer\": \"a\",\n\
    \"category\": \"{subject_lowercase}\"\n\
}}\n\
\n\
Return only the JSON array, no additional text."

# Backend API Port (if running main.py directly)
# PORT=8000
# HOST="0.0.0.0"
